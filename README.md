# ğŸŒ¾ KCC Query Assistant

A fully local, multilingual AI assistant for answering agricultural queries using real Q&A pairs from government datasets (Krishi Call Center - KCC).

This project uses Retrieval-Augmented Generation (RAG) with **ChromaDB** and **Ollama** (`gemma:2b`) to run **completely offline** â€” no cloud, no API calls.

---

## ğŸš€ Features

- âœ… Local embeddings with `sentence-transformers` (`MiniLM`)
- âœ… Vector search using ChromaDB
- âœ… Local LLM response generation via Ollama (`gemma:2b`)
- âœ… Streamlit UI for easy Q&A interaction
- âœ… Graceful fallback when no context is found

---

## ğŸ› ï¸ Technologies

- Python 3.10
- [sentence-transformers](https://www.sbert.net/)
- [ChromaDB](https://www.trychroma.com/)
- [Ollama](https://ollama.com/) + `gemma:2b`
- Streamlit

---

## ğŸ“‚ Project Structure
- `app.py`  
  â†’ Streamlit web UI to enter queries and show results

- `query_rag.py`  
  â†’ Loads ChromaDB, retrieves top-k results, and calls Ollama for response

- `embed_store.py`  
  â†’ Embeds `kcc_chunks.json` into ChromaDB and saves the vector store locally

- `KCC_data_process.ipynb`  
  â†’ Jupyter notebook for cleaning and transforming raw KCC CSV data into Q&A format

- `kcc_chunks.json`  
  â†’ Final processed Q&A dataset used for vector search

- `requirements.txt`  
  â†’ List of Python packages needed to run the project

- `sample_queries.txt`  
  â†’ Contains 9 test questions, 5 without context  
    - ğŸ§ª **Note on Test Questions**  
      This file includes **3 test queries with valid KCC context** and **2 queries without matching context** to demonstrate fallback behavior.  
      ğŸ‘‰ **Refer to the demo video** for input/output examples and expected responses for all 5.


---

## â–¶ï¸ How to Run the Project

### 1. Install Python dependencies

```bash
pip install -r requirements.txt
```

### 2. Start Ollama with local LLM

- `chroma_store/` *(not pushed to GitHub)*  
  â†’ Auto-generated Chroma vector database directory (recreated by `embed_store.py`)

```bash
ollama run gemma:2b
```
>>> Send a message (/? for help)

- Press Ctrl + D to exit the interactive session and move on to the next step.


### 3. Run the Streamlit app

```bash
streamlit run app.py
```

- This will launch the app in your default browser.
If it doesn't open automatically, visit: http://localhost:8501

---

## ğŸ“½ï¸ Demo Video  
ğŸ”— [Click here to watch demo](https://drive.google.com/file/d/1hglyiJi6P-5Uyz4OPLm6l1qOybW-3vSn/view?usp=sharing)  <!-- Replace # with actual video link -->

Includes:
- âœ… Local Ollama + ChromaDB startup
- âœ… 3â€“5 working queries using KCC context
- âœ… 2â€“3 fallback examples (no context matched)
- âœ… All responses generated **100% offline**



