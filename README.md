# ğŸŒ¾ KCC Query Assistant

A fully local, multilingual AI assistant for answering agricultural queries using real Q&A pairs from government datasets (Krishi Call Center - KCC).

This project uses Retrieval-Augmented Generation (RAG) with **ChromaDB** and **Ollama** (`gemma:2b`) to run **completely offline** â€” no cloud, no API calls.

---

## ğŸš€ Features

- âœ… Local embeddings with `sentence-transformers` (`MiniLM`)
- âœ… Vector search using ChromaDB
- âœ… Local LLM response generation via Ollama (`gemma:2b`)
- âœ… Streamlit UI for easy Q&A interaction
- âœ… Graceful fallback when no context is found

---

## ğŸ› ï¸ Technologies

- Python 3.10
- [sentence-transformers](https://www.sbert.net/)
- [ChromaDB](https://www.trychroma.com/)
- [Ollama](https://ollama.com/) + `gemma:2b`
- Streamlit

---

## ğŸ“‚ Project Structure
- `app.py`  
  â†’ Streamlit web UI to enter queries and show results

- `query_rag.py`  
  â†’ Loads ChromaDB, retrieves top-k results, and calls Ollama for response

- `embed_store.py`  
  â†’ Embeds `kcc_chunks.json` into ChromaDB and saves the vector store locally

- `KCC_data_process.ipynb`  
  â†’ Jupyter notebook for cleaning and transforming raw KCC CSV data into Q&A format

- `kcc_chunks.json`  
  â†’ Final processed Q&A dataset used for vector search

- `requirements.txt`  
  â†’ List of Python packages needed to run the project

- `sample_queries.txt`  
  â†’ Contains 10 test questions: 5 with answers, 5 without context

- `chroma_store/` *(not pushed to GitHub)*  
  â†’ Auto-generated Chroma vector database directory (recreated by `embed_store.py`)
